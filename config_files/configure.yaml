BasicSettings:
  ImageSize: 64
  ImageChannel: 3
  ReplayBufferOnGPU: True
  Seed: 3710
  Env_name: ALE/Pong-v5
  Device: cuda:0
  Use_amp: True
  Use_cg: True
  Compile: True
  SavePath: None

  CausalGradCheckpoint: True    # Saves 40% VRAM
  CausalChunkSize: 8            # For long sequences

Evaluate:
   EpisodeNum: 10
   NumEnvs: 10
   DuringTraining: True
   EverySteps: 1000

JointTrainAgent:
  SampleMaxSteps: 105000 # Just to make sure the last episode will finish, no training after 100k
  BufferMaxLength: 100000
  WorldModelWarmUp: 1032
  BehaviourWarmUp: 1032
  NumEnvs: 1
  BatchSize: 16
  BatchLength: 128
  ImagineBatchSize: 1024
  ImagineContextLength: 8
  ImagineBatchLength: 16
  RealityContextLength: 16
  TrainDynamicsEverySteps: 1
  TrainDynamicsEpoch: 1
  TrainAgentEverySteps: 1
  FreezeWorldModelAfterSteps: 100000
  FreezeBehaviourAfterSteps: 100000
  SaveEverySteps: 2000
  SaveModels: True
  Tau: 10
  ImaginationTau: 10
  Alpha: 1.0 # High focus on penalising high imagine counts regardless of train counts, less probability to be sampled
  Beta: 1.0 # High focus on penalising

  CausalWarmUp: 2000              # Causal model pretraining
  AlternateTrainEvery: 5          # Alternate WM/causal training

Models:
  WorldModel:
    dtype: float32
    Backbone: Mamba2 # Mamba, Mamba2, Transformer
    InChannels: 3
    Act: SiLU
    CategoricalDim: 32
    ClassDim: 32
    HiddenStateDim: 512      # Must be greater than CodeDim *2 + Confdim
    Optimiser: Laprop
    LatentDiscreteType: naive
    Max_grad_norm: 1000
    Warmup_steps: 1000
    Dropout: 0.1
    Unimix_ratio: 0.01
    Weight_decay: 1.0e-4
    Adam:
      LearningRate: 1.0e-4
    Laprop:
      LearningRate: 4.0e-5
      Epsilon: 1.0e-20
    Encoder:
      Depth: 16
      Mults:  [1, 2, 3, 4, 4]
      Norm: rms
      Kernel: 5
      Padding: same
      InputSize: [3, 64, 64]
    Decoder:
      Depth: 16
      Mults:  [1, 2, 3, 4, 4]
      Norm: rms
      Kernel: 5
      Padding: same
      FirstStrideOne: True
      InputSize: [3, 64, 64]
      FinalLayerSigmoid: True
    Reward:
      HiddenUnits: 256
      LayerNum: 1
    Termination:
      HiddenUnits: 256
      LayerNum: 1      
    Transformer:
      FinalFeatureWidth: 4
      NumLayers: 2
      NumHeads: 8
    Mamba:
      n_layer: 2
      d_intermediate: 0
      ssm_cfg:
        d_state: 16

  CausalModel:
    dtype: float32
    CodeDim: 64               # Dimension of codebook entries
    ConfDim: 32               # Confounder dimension
    NumCodesTr: 512           # Transition codebook size
    NumCodesRe: 256           # Reward codebook size
    HiddenDim: 256            # Hidden dimension for MLPs
    KLWeight: 0.1             # KL Divergence loss weight
    QuantWeight: 0.5          # Quantization loss weight
    RegWeight: 0.01           # Regularization loss weight
    InvarianceWeight: 0.3     # Mechanism invariance loss
    SparsityWeight: 0.1       # Mask sparsity regularization

    Quantizer:
      Temperature: 1.0        # Initial Gumbel softmax temp
      AnnealFactor: 0.95      # Temp decay per update
      MinTemperature: 0.1     # Lower temperature bound
      Beta: 0.25              # Commitment loss weight
      NormalizedInputs: True  # L2 normalize inputs/codebook
      Coupling: True          # Enable codebook coupling
      LambdaCouple: 0.1       # Coupling loss weight

    Confounder:
      PriorLayers: 2          # MLP layers for prior network
      PostLayers: 2           # MLP layers for posterior
      AffineInitStd: 0.01     # Affine param initialization
      MinLogVar: -6.0         # Posterior variance bounds
      MaxLogVar: 0.0

    Modulation:
      MaskInitScale: 0.5      # Initial scale for modulation
      MaskSparsity: 0.8       # Target sparsity ratio
      MinScale: 0.1           # Clamp scale values
      MaxScale: 1.9           # Uppder clamp scale value

    Integration:
      GradDetachWorld: True   # Detach world model gradients
      AnnealSteps: 1000       # Mask sparsity warmup

  Agent:
    dtype: float32
    Policy: AC # AC or PPO
    Unimix_ratio: 0
    AC:
      NumLayers: 3    
      Gamma: 0.985
      Lambda: 0.95
      EntropyCoef: 3.e-4
      Max_grad_norm: 100
      Warmup_steps: 1000
      Act: SiLU
      Optimiser: Laprop
      Adam:
        LearningRate: 3.0e-5
        Epsilon: 1.0e-5
      Laprop:
        LearningRate: 4.0e-5
        Epsilon: 1.0e-20
      Actor:
        HiddenUnits: 256
      Critic:
        HiddenUnits: 512  
    PPO:
      NumLayers: 3
      Gamma: 0.985
      Lambda: 0.95
      EpsilonClip: 0.2
      K_epochs: 3
      Minibatch: 16384
      CriticCoef: 1
      EntropyCoef: 3.e-4
      KL_threshold: 0.01
      Max_grad_norm: 100
      Warmup_steps: 1000
      Act: SiLU
      Optimiser: Laprop
      Adam:
        LearningRate: 3.e-5
        Epsilon: 1.0e-5
      Laprop:
        LearningRate: 4.0e-5
        Epsilon: 1.0e-20
      Actor:
        HiddenUnits: 256
      Critic:
        HiddenUnits: 512 # Andrychowicz2020 wider critic network seems better  
Wandb:
  Init:
    Mode: online
    Project: Mamba_dreamer
n: standard